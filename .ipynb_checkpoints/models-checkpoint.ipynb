{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CiwtNgENbx2g"
   },
   "source": [
    "# Google Smart Composer\n",
    "\n",
    "Google Smart Composer is a machine learning model that generates interactive word(s) suggestions. Given to whom you are writing the email, subject of the email previous email content(if a replied email) and a small part of the content of the email then the next coming word(s) is predicted by the model\n",
    "\n",
    "<img src=\"hedder.png\" width=\"1000\" height=\"480\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wfodePkj3jEa"
   },
   "source": [
    "## Data: \n",
    "Data is taken from personal emails and constructed according to problem specific.\n",
    "\n",
    "### Structure of data:\n",
    "From all the emails 'to', 'subject', 'previous email'(if any) and 'content' parts are taken. As we are going to predict next comming word in the content  part, content part is breaked into following way. We are restriction our self to predict only atmost 5 next comming words so each email content will be breaked to many sentances.\n",
    "<br><b>Ex:</b> Content: This is introduction to my project<br>\n",
    " <table>\n",
    "    <tr>\n",
    "      <th>Sentance</th>\n",
    "      <th>Output</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td> This </td>\n",
    "      <td> is</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td> This </td>\n",
    "      <td> is introduction </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td> This </td>\n",
    "      <td> is introduction to</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td> This </td>\n",
    "      <td> is introduction to my</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td> This </td>\n",
    "      <td> is introduction to my project</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td> This is </td>\n",
    "      <td> introduction</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td> This is</td>\n",
    "      <td> introduction to </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td> This is </td>\n",
    "      <td> introduction to my</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td> This is</td>\n",
    "      <td> introduction to my</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td> This is</td>\n",
    "      <td> introduction to my project</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td> This is introduction</td>\n",
    "      <td> to</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td> This is introduction</td>\n",
    "      <td>  to my </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td> This is introduction </td>\n",
    "      <td> to my project</td>\n",
    "    </tr>\n",
    "    \n",
    "   \n",
    " </table>\n",
    "\n",
    "Now to the sentance part 'to','subject', 'previous email' parts are joined with their corresponding separaters.<br>\n",
    "<b>Ex:</b> < to > email@ email.com< sub > introduction < prv > nan < cont > hello email this is only an intro \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libriries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tnxXKDjq3jEL",
    "outputId": "939dad71-d5ad-4615-8d1c-1055f26adf64"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "OHn4Dct23jEm",
    "outputId": "a61ff945-c717-4ecc-dee5-80937adab43f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('gdrive',force_remount=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ea3MrnQGDWEE"
   },
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "colab_type": "code",
    "id": "LSGthirAeNPh",
    "outputId": "d8f0829b-db7b-4d1c-ed07-e1c7f363e9f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in data 37230\n",
      "Number of columns in data 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;to&gt; yernagulahemanth &lt;prv&gt; nan &lt;sub&gt;  about f...</td>\n",
       "      <td>yernagulahemanth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;to&gt; yernagulahemanth &lt;prv&gt; nan &lt;sub&gt;  about f...</td>\n",
       "      <td>yernagulahemanth can</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;to&gt; yernagulahemanth &lt;prv&gt; nan &lt;sub&gt;  about f...</td>\n",
       "      <td>yernagulahemanth can you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;to&gt; yernagulahemanth &lt;prv&gt; nan &lt;sub&gt;  about f...</td>\n",
       "      <td>yernagulahemanth can you explain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;to&gt; yernagulahemanth &lt;prv&gt; nan &lt;sub&gt;  about f...</td>\n",
       "      <td>yernagulahemanth can you explain more</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   x                                      y\n",
       "0  <to> yernagulahemanth <prv> nan <sub>  about f...                       yernagulahemanth\n",
       "1  <to> yernagulahemanth <prv> nan <sub>  about f...                   yernagulahemanth can\n",
       "2  <to> yernagulahemanth <prv> nan <sub>  about f...               yernagulahemanth can you\n",
       "3  <to> yernagulahemanth <prv> nan <sub>  about f...       yernagulahemanth can you explain\n",
       "4  <to> yernagulahemanth <prv> nan <sub>  about f...  yernagulahemanth can you explain more"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('gdrive/My Drive/google/final_data_my_emails.csv')\n",
    "print('Number of rows in data',data.shape[0])\n",
    "print('Number of columns in data',data.shape[1])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "cTbSbBz55QtF",
    "outputId": "0947b7ac-d88a-406f-f0d4-73b1ea28f686"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;start&gt; &lt;to&gt; yernagulahemanth &lt;prv&gt; nan &lt;sub&gt; ...</td>\n",
       "      <td>&lt;start&gt; yernagulahemanth &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;start&gt; &lt;to&gt; yernagulahemanth &lt;prv&gt; nan &lt;sub&gt; ...</td>\n",
       "      <td>&lt;start&gt; yernagulahemanth can &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;start&gt; &lt;to&gt; yernagulahemanth &lt;prv&gt; nan &lt;sub&gt; ...</td>\n",
       "      <td>&lt;start&gt; yernagulahemanth can you &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;start&gt; &lt;to&gt; yernagulahemanth &lt;prv&gt; nan &lt;sub&gt; ...</td>\n",
       "      <td>&lt;start&gt; yernagulahemanth can you explain &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;start&gt; &lt;to&gt; yernagulahemanth &lt;prv&gt; nan &lt;sub&gt; ...</td>\n",
       "      <td>&lt;start&gt; yernagulahemanth can you explain more ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   x                                                  y\n",
       "0  <start> <to> yernagulahemanth <prv> nan <sub> ...                     <start> yernagulahemanth <end>\n",
       "1  <start> <to> yernagulahemanth <prv> nan <sub> ...                 <start> yernagulahemanth can <end>\n",
       "2  <start> <to> yernagulahemanth <prv> nan <sub> ...             <start> yernagulahemanth can you <end>\n",
       "3  <start> <to> yernagulahemanth <prv> nan <sub> ...     <start> yernagulahemanth can you explain <end>\n",
       "4  <start> <to> yernagulahemanth <prv> nan <sub> ...  <start> yernagulahemanth can you explain more ..."
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.y = data.y.apply(lambda x:str(x)) # y part was having now int values so they are converted to string values\n",
    "\n",
    "# Along with defalt tags x and y are added with <start> and <end> tags at starting and ending \n",
    "data.x = data.x.apply(lambda x:'<start> ' + str(x) + ' <end>')\n",
    "data.y = data.y.apply(lambda x:'<start> ' + str(x) + ' <end>')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qqsxUmfBYfpr"
   },
   "outputs": [],
   "source": [
    "# Tokenizing x and y\n",
    "# Tokenizing: Collecting all tokens(words) for x and y and storing them  with index number\n",
    "\n",
    "X_  = list(data.x.values)\n",
    "Y_ = list(data.y.values)\n",
    "\n",
    "\n",
    "\n",
    "# Tokenizing x values, since default values of filters in tokenizer is special charecters we are replacing that \n",
    "# default value by ''\n",
    "x_tokenizer   = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "x_tokenizer.fit_on_texts(X_)\n",
    "x_tensor         = x_tokenizer.texts_to_sequences(X_)\n",
    "\n",
    "# padding\n",
    "x_tensor         = tf.keras.preprocessing.sequence.pad_sequences(x_tensor,padding='post')\n",
    "\n",
    "\n",
    "y_tokenizer  = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "y_tokenizer.fit_on_texts(Y_)\n",
    "y_tensor        = y_tokenizer.texts_to_sequences(Y_)\n",
    "y_tensor        = tf.keras.preprocessing.sequence.pad_sequences(y_tensor,padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hH4ByHDQBtAE"
   },
   "outputs": [],
   "source": [
    "# Y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "Lm4BF5aKvK1Z",
    "outputId": "f8f35a53-c273-4dee-f09a-13db40c471f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in x: 1449\n",
      "Total number of words in y: 1417\n"
     ]
    }
   ],
   "source": [
    "print('Total number of words in x:',len(x_tokenizer.index_word))\n",
    "print('Total number of words in y:',len(y_tokenizer.index_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "4QILQkOs3jFG",
    "outputId": "eeefba74-0cec-4cd2-ba0d-013a5975dd03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points in xtrain: 29784\n",
      "Number of data points in ytrain: 29784\n",
      "Number of data points in xval  : 7446\n",
      "Number of data points in yval  : 7446\n"
     ]
    }
   ],
   "source": [
    "# Making train and validation data with 80-20 ratio \n",
    "x_train, x_val, y_train, y_val = train_test_split(x_tensor, y_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "y_token_max_len, x_token_max_len = max([len(i) for i in y_tensor]), max([len(i) for i in x_tensor])\n",
    "\n",
    "print('Number of data points in xtrain:',len(x_train))\n",
    "print('Number of data points in ytrain:',len(y_train))\n",
    "print('Number of data points in xval  :',len(x_val))\n",
    "print('Number of data points in yval  :',len(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XK23lXf95lNj"
   },
   "outputs": [],
   "source": [
    "y_token_max_len,x_token_max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YwfyKgwFEwIQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rgCLkfv5uO3d"
   },
   "source": [
    "## Feeding this data into tensorflow data api\n",
    "we can make operation like shuffleing, getting batch wise so as to make future simple\n",
    "\n",
    "`Ref: https://www.tensorflow.org/guide/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TqHsArVZ3jFS"
   },
   "outputs": [],
   "source": [
    "Batch_Size = 50\n",
    "steps_per_epoch = len(x_train)//Batch_Size\n",
    "Emd_dim = 200\n",
    "units = 500\n",
    "x_vocab_size = len(x_tokenizer.word_index)+1\n",
    "y_vocab_size = len(y_tokenizer.word_index)+1\n",
    "\n",
    "# Shuffling the data set \n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(len(x_train))\n",
    "# Getting batch wise and droping last batch if it is not having less number of points(diss similar batch)\n",
    "dataset = dataset.batch(Batch_Size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qc6-NK1GtWQt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TNfHIF71ulLu"
   },
   "source": [
    "## Encoder Decoder With ATTENTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"model archi.png\" width=\"1000\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<strong>Short Explination:</strong>\n",
    "<ol>\n",
    "    <li>The 'to', 'subject', 'previous email', 'current email prefix' is concatenated.</li>\n",
    "    <li>This sequence is sent into sequence model in our case encoder, decoder with attention</li>\n",
    "    <li>This model predicts the next comming words after current email prefix</li>\n",
    "</ol>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder  Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Every time when a sentance is passed to the encoder, at every time step the outputs are depricated and hidden states from every time steps pass to next time step at the end it generates output and a weight vector which hold whole context of the sentance</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"encoder1.png\" width=\"1000\" height=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nZ2rI24i3jFg"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "  \n",
    "  def __init__(self, vocab_size, Emd_dim, enc_units, batch_size):\n",
    "    '''\n",
    "    vocab_size   - Vocabulary size\n",
    "    Emd_dim      - Number of emding dimensions\n",
    "    batch_size   - Batch Size\n",
    "    \n",
    "    '''\n",
    "    super(Encoder, self).__init__()\n",
    "    self.batch_size = batch_size\n",
    "    self.enc_units = enc_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, Emd_dim)\n",
    "    self.encoder = tf.keras.layers.LSTM(self.enc_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "  def call(self, input_, hidden):\n",
    "\n",
    "    '''\n",
    "      Given input and hidden states for encoder returns the updated hidden states and output\n",
    "    '''\n",
    "    input_ = self.embedding(input_)\n",
    "    # print(x.shape,hidden[0].shape,hidden[1].shape)\n",
    "    output, state_h,state_c = self.encoder(input_, initial_state = hidden)\n",
    "    # state = tf.concat([state_h,state_c],axis=1)\n",
    "    state = [state_h,state_c]\n",
    "    return output, state\n",
    "\n",
    " \n",
    "  def initialize_hidden_state(self):\n",
    "      '''\n",
    "       Returns initial states for encoder (zeros)\n",
    "      '''\n",
    "      initial_states = [tf.zeros((self.batch_size, self.enc_units)),tf.zeros((self.batch_size, self.enc_units))]\n",
    "      return initial_states\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "60gSVh05Jl6l"
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(x_vocab_size, Emd_dim, units, Batch_Size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> In each time step the decoder hidden state and encoder outputs are passed through dense layer so that they get their weights and can be trained through back propagation,  both this weights passed from a dense layer is passed through tanh activation function and than passed through dense unit with single activation unit upon which softmax activation is applied so that sum of the  vector will be sum to one  which is known as attention weights</p>\n",
    "\n",
    "<p>Now this attention weights are element wise multiplied encoder output which produce context vector, this context vector will be holding all important words with high values and less important words with low values as we have applied softmax activation function</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src = \"encoder_attention_decoder.png\" width=\"1000\" height=\"500\">\n",
    "    <h6 align=\"center\">Image courtesy : https://blog.floydhub.com/attention-mechanism/.</h6>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "umohpBN2OM94"
   },
   "outputs": [],
   "source": [
    "class Attention(tf.keras.layers.Layer):\n",
    "  def __init__(self, units):\n",
    "    super(Attention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "    self.units = units\n",
    "\n",
    "  def call(self, hid, enc_out):\n",
    "\n",
    "    # hidden shape == (Batch_Size, hidden size)\n",
    "    # hidden_with_time_axis shape == (Batch_Size, 1, hidden size)\n",
    "    # we are doing this to perform addition to calculate the score\n",
    "    # hidden_with_time_axis = tf.expand_dims(hid, 1)\n",
    "    # Since Lstm layers will give us two states i.e cell state and hidden state \n",
    "    # we are passing hidden state through dense layer and cell state through dence state\n",
    "    # and adding both outputs of dence  layer\n",
    "    hidden_with_time_axis  = tf.expand_dims(self.W2(hid[0])+self.W2(hid[1]),1)\n",
    "    # print(hidden_with_time_axis.shape)\n",
    "    # Now this sum of  outputs from dense layers is added with outputs of  encoder output and \n",
    "    # along with tanh and a single unit dense layer\n",
    "    # score shape == (Batch_Size, max_length, 1)\n",
    "    # we get 1 at the last axis because we are applying score to self.V\n",
    "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "    score = self.V(tf.nn.tanh(\n",
    "        self.W1(enc_out) + hidden_with_time_axis))\n",
    "\n",
    "    # attention_weights shape == (batch_size, max_length, 1)\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "    context_vector = attention_weights * enc_out\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k534zTHiDjQU"
   },
   "outputs": [],
   "source": [
    "attention_layer = Attention(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> For the first time step the context vector generated with encoder output and encoder hidden states are passed  to the decoder initial  stage. From next time step onwards  the  output producted from decoder in previous time step and hidden states for decoder in previous time step are passed to attention layer which generates context vector, then this context vector concatnated with decoder  input and passed into the decoder layer, this process happens till the end tag (< end >) reached in the sequence<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"decoder1.png\" width=\"1000\" height=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yJ_B3mhW3jFk"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, Emd_dim, dec_units, batch_size,nwtpa = 10):\n",
    "    ''' \n",
    "    All the variables are same as previous encoder calss except nwtpa\n",
    "    nwtpa is number of units you want to maintain at attention mechanism\n",
    "    '''\n",
    "\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_size = batch_size\n",
    "    self.dec_units = dec_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, Emd_dim)\n",
    "    self.decoder = tf.keras.layers.LSTM(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "    self.nwtpa = nwtpa\n",
    "\n",
    "    # used for attention\n",
    "    self.attention = Attention(self.nwtpa)\n",
    "\n",
    "  def call(self, input_, hidden, enc_output):\n",
    "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "    # passing  hidden states and encoder output to the attention layer\n",
    "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "    # applying embdding to the inputs of the dense layer\n",
    "    # input_ shape after passing through embedding == (batch_size, 1, Emd_dim)\n",
    "    input_ = self.embedding(input_)\n",
    "\n",
    "    # concating previous input and context vector\n",
    "    # input_ shape after concatenation == (batch_size, 1, Emd_dim + hidden_size)\n",
    "    input_ = tf.concat([tf.expand_dims(context_vector, 1), input_], axis=-1)\n",
    "\n",
    "    # passing the concatenated vector to the Lstm\n",
    "    output, state_h,state_c = self.decoder(input_)\n",
    "    state = [state_h,state_c]\n",
    "\n",
    "    # output shape == (batch_size * 1, hidden_size)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size, vocab)\n",
    "    output = self.fc(output)\n",
    "\n",
    "    return output, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P5UY8wko3jFp"
   },
   "outputs": [],
   "source": [
    "decoder = Decoder(y_vocab_size, Emd_dim, units, Batch_Size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_ch_71VbIRfK"
   },
   "source": [
    "## Define the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WmTHr5iV3jFr"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(actual, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(actual, 0))\n",
    "  loss_ = loss_object(actual, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DMVWzzsfNl4e"
   },
   "source": [
    "## Creating Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zj8bXQTgNwrF"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hpObfY22IddU"
   },
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LCAOh2d-evWO"
   },
   "outputs": [],
   "source": [
    "# for i in os.listdir('training_checkpoints'):\n",
    "#     os.remove('training_checkpoints/'+i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ddefjBMa3jF0"
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(EPOCHS = 10,verbose = 0):\n",
    "     \n",
    "      ''' \n",
    "          Given number of epochs and verbose model is trainied with those number of epochs\n",
    "          verbose - If verbose is 0 no information about batches is prednted, if verbose is given\n",
    "                    everytime when batch reaches batch_size/verbose info is printed\n",
    "      '''\n",
    "      for epoch in range(EPOCHS):\n",
    "            start = time.time()\n",
    "            \n",
    "            # initial hidden states for encoder is being generated\n",
    "            enc_hidden = encoder.initialize_hidden_state()\n",
    "            total_loss = 0\n",
    "\n",
    "            for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "                    # batch_loss = train_step(inp, targ, enc_hidden)\n",
    "                    # total_loss += batch_loss\n",
    "                    loss = 0\n",
    "                  \n",
    "                    with tf.GradientTape() as tape:\n",
    "                            \n",
    "                          # encoder hidden state and x(inp) is passed to encoder which in return gives\n",
    "                          # encoder output and its hidden states\n",
    "                          enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "                          dec_hidden = enc_hidden\n",
    "                            \n",
    "                          # for the first time decoder input will be only <start> tag\n",
    "\n",
    "                          dec_input = tf.expand_dims([y_tokenizer.word_index['<start>']] * Batch_Size, 1)\n",
    "                          for t in range(1, targ.shape[1]):\n",
    "                              # passing enc_output, dec_input, dec_hidden state to the decoder\n",
    "                              predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "                              loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "                              # using teacher forcing\n",
    "                              dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "                    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "                    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "                    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "                    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "                    if verbose:\n",
    "                        if batch % verbose == 0:\n",
    "                              print('Epoch {} Batch {} Loss {}'.format(epoch + 1, batch,batch_loss.numpy()))\n",
    "                                \n",
    "            # saving (checkpoint) the model every 2 epochs\n",
    "            if (epoch + 1) % 2 == 0:\n",
    "              checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "            print('Epoch {} Loss {}'.format(epoch + 1, total_loss / steps_per_epoch))\n",
    "            print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ARZzEKBbTivh",
    "outputId": "b128979e-81a5-468c-df06-c2a8c1f0ea6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.1264\n",
      "Epoch 1 Batch 100 Loss 2.9349\n",
      "Epoch 1 Batch 200 Loss 2.7352\n",
      "Epoch 1 Batch 300 Loss 3.0306\n",
      "Epoch 1 Batch 400 Loss 3.0160\n",
      "Epoch 1 Batch 500 Loss 2.6029\n",
      "Epoch 1 Loss 0.0\n",
      "Time taken for 1 epoch 127.41328072547913 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 2.1338\n",
      "Epoch 2 Batch 100 Loss 2.1332\n",
      "Epoch 2 Batch 200 Loss 1.9065\n",
      "Epoch 2 Batch 300 Loss 1.9896\n",
      "Epoch 2 Batch 400 Loss 1.5953\n",
      "Epoch 2 Batch 500 Loss 1.6653\n",
      "Epoch 2 Loss 0.0\n",
      "Time taken for 1 epoch 112.77655529975891 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.5814\n",
      "Epoch 3 Batch 100 Loss 1.5359\n",
      "Epoch 3 Batch 200 Loss 1.5008\n",
      "Epoch 3 Batch 300 Loss 1.4783\n",
      "Epoch 3 Batch 400 Loss 1.4984\n",
      "Epoch 3 Batch 500 Loss 1.3349\n",
      "Epoch 3 Loss 0.0\n",
      "Time taken for 1 epoch 112.18088150024414 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.2326\n",
      "Epoch 4 Batch 100 Loss 1.2452\n",
      "Epoch 4 Batch 200 Loss 1.1833\n",
      "Epoch 4 Batch 300 Loss 1.2856\n",
      "Epoch 4 Batch 400 Loss 1.2822\n",
      "Epoch 4 Batch 500 Loss 1.1809\n",
      "Epoch 4 Loss 0.0\n",
      "Time taken for 1 epoch 114.6331856250763 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.2219\n",
      "Epoch 5 Batch 100 Loss 1.1174\n",
      "Epoch 5 Batch 200 Loss 1.0799\n",
      "Epoch 5 Batch 300 Loss 1.1781\n",
      "Epoch 5 Batch 400 Loss 1.0822\n",
      "Epoch 5 Batch 500 Loss 0.9566\n",
      "Epoch 5 Loss 0.0\n",
      "Time taken for 1 epoch 113.33757972717285 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.9424\n",
      "Epoch 6 Batch 100 Loss 0.9333\n",
      "Epoch 6 Batch 200 Loss 0.9851\n",
      "Epoch 6 Batch 300 Loss 0.9313\n",
      "Epoch 6 Batch 400 Loss 0.9915\n",
      "Epoch 6 Batch 500 Loss 0.9855\n",
      "Epoch 6 Loss 0.0\n",
      "Time taken for 1 epoch 113.54746222496033 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.8394\n",
      "Epoch 7 Batch 100 Loss 0.9163\n",
      "Epoch 7 Batch 200 Loss 0.8759\n",
      "Epoch 7 Batch 300 Loss 0.8819\n",
      "Epoch 7 Batch 400 Loss 0.8631\n",
      "Epoch 7 Batch 500 Loss 0.8246\n",
      "Epoch 7 Loss 0.0\n",
      "Time taken for 1 epoch 112.03907346725464 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.8177\n",
      "Epoch 8 Batch 100 Loss 0.7740\n",
      "Epoch 8 Batch 200 Loss 0.7412\n",
      "Epoch 8 Batch 300 Loss 0.8472\n",
      "Epoch 8 Batch 400 Loss 0.8170\n",
      "Epoch 8 Batch 500 Loss 0.8224\n",
      "Epoch 8 Loss 0.0\n",
      "Time taken for 1 epoch 111.9318687915802 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.7100\n",
      "Epoch 9 Batch 100 Loss 0.8428\n",
      "Epoch 9 Batch 200 Loss 0.7434\n",
      "Epoch 9 Batch 300 Loss 0.7513\n",
      "Epoch 9 Batch 400 Loss 0.8366\n",
      "Epoch 9 Batch 500 Loss 0.8006\n",
      "Epoch 9 Loss 0.0\n",
      "Time taken for 1 epoch 111.95030188560486 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.6798\n",
      "Epoch 10 Batch 100 Loss 0.7494\n",
      "Epoch 10 Batch 200 Loss 0.7031\n",
      "Epoch 10 Batch 300 Loss 0.6558\n",
      "Epoch 10 Batch 400 Loss 0.6665\n",
      "Epoch 10 Batch 500 Loss 0.6705\n",
      "Epoch 10 Loss 0.0\n",
      "Time taken for 1 epoch 112.42571496963501 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(verbose=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EbQpyYs13jF_"
   },
   "outputs": [],
   "source": [
    "def test(sentence):\n",
    "  attention_plot = np.zeros((y_token_max_len, x_token_max_len))\n",
    "  \n",
    "  # Tokenizing inputs\n",
    "  inputs = [x_tokenizer.word_index[i] for i in sentence.split()]\n",
    "  # Padding inputs\n",
    "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=x_token_max_len,\n",
    "                                                         padding='post')\n",
    "  inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "  result = ''\n",
    "\n",
    "  # Making initial hidden states for encoder\n",
    "  hidden = [tf.zeros((1, units)),tf.zeros((1, units))]\n",
    "  \n",
    "  # inputs and hidden states are passed to encoder\n",
    "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "  dec_hidden = enc_hidden\n",
    "\n",
    "  # Making initial inputs for decoder as <start> tag\n",
    "  dec_input = tf.expand_dims([y_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "  for t in range(y_token_max_len):\n",
    "          # inputing decoder input(<start> during initial sate), encoder hidden state and encoder output\n",
    "          predictions, dec_hidden,_ = decoder(dec_input, dec_hidden,enc_out)\n",
    "\n",
    "          predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "          \n",
    "          # the predicted word is appended to result\n",
    "        \n",
    "          result += y_tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "          # if the predicted word is <end> returning the words predicted till now\n",
    "          if y_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result\n",
    "\n",
    "          # current output from decoder is fed back to decoder into next timestep\n",
    "          dec_input = tf.expand_dims([predicted_id], 0)\n",
    "          \n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s5hQWlbN3jGF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sl9zUHzg3jGI"
   },
   "outputs": [],
   "source": [
    "def predict_next_word(sentence):\n",
    "  result = test(sentence)\n",
    "  return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n250XbnjOaqP"
   },
   "source": [
    "## Restore the latest checkpoint and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UJpT9D5_OgP6",
    "outputId": "d92c430b-1380-4416-e943-1c2924490e6e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fd1699abb38>"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint_dir = 'gdrive/My Drive/google/check_points'\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QP0u4yp4i5w4"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "# Moving checkpoints to drive\n",
    "for i in os.listdir('training_checkpoints'):\n",
    "  shutil.move('training_checkpoints/'+i,'gdrive/My Drive/google/check_points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WrAM0FDomq3E"
   },
   "outputs": [],
   "source": [
    "predict_next_word(u'<start> <to> team <prv> nan <sub> about mentor <cont> hello team i recently completed my first mot interview will there be any mentor guiding us for the remaining part of our course till getting into the job <end>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FC4iUQTBhabQ"
   },
   "source": [
    "## We shall check the  model with some unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z3GHH2_IhaAX"
   },
   "outputs": [],
   "source": [
    "def generate_sent(id_):\n",
    "\n",
    "        test_with = []\n",
    "        try:\n",
    "          for i in x_val[id_]:\n",
    "            test_with.append(x_tokenizer.index_word[i])\n",
    "        except:\n",
    "\n",
    "          return ' '.join(test_with)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "colab_type": "code",
    "id": "0RDhENMehZ9o",
    "outputId": "53cde6e1-9533-49ef-ba3e-191c8e65ee61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- input ----------\n",
      "<start> <to> team <prv> nan <sub> regarding new class room <cont> hello team we are migrating classrooms one by one you will receive a communication <end>\n",
      "\n",
      "\n",
      "---------- actual ----------\n",
      "  classrooms one by one you will receive a communication  \n",
      "\n",
      "\n",
      "---------- predicted ----------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'classrooms one by one <end> '"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ = generate_sent(5)\n",
    "print('-'*10,'input','-'*10)\n",
    "print(input_)\n",
    "print('\\n')\n",
    "print('-'*10,'actual','-'*10)\n",
    "print('  classrooms one by one you will receive a communication  ')\n",
    "print('\\n')\n",
    "print('-'*10,'predicted','-'*10)\n",
    "predict_next_word('<start> <to> team <prv> nan <sub> regarding new class room <cont> hello team we are migrating <end>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "colab_type": "code",
    "id": "AXOp0ZIuoom5",
    "outputId": "bb39cb9e-c413-45b5-9dee-bd0de8fcefac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- input ----------\n",
      "<start> <to> saitejapsk <prv> nan <sub> new netflix series <cont> hello saitejapsk i m thinking of getting netflix what s the first thing i should add to <end>\n",
      "\n",
      "\n",
      "---------- actual ----------\n",
      "  netflix what s the first thing i should add to  \n",
      "\n",
      "\n",
      "---------- predicted ----------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'netflix what s the first <end> '"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ = generate_sent(128)\n",
    "print('-'*10,'input','-'*10)\n",
    "print(input_)\n",
    "print('\\n')\n",
    "print('-'*10,'actual','-'*10)\n",
    "print('  netflix what s the first thing i should add to  ')\n",
    "print('\\n')\n",
    "print('-'*10,'predicted','-'*10)\n",
    "predict_next_word('<start> <to> saitejapsk <prv> nan <sub> new netflix series <cont> hello saitejapsk i m thinking of getting  <end>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "colab_type": "code",
    "id": "2dzlTwcPrUWq",
    "outputId": "f1b422ef-96d9-4537-d7bb-b24e99ad4b6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- input ----------\n",
      "<start> <to> yernagulahemanth <prv> hello yernagulahemanth wow this is great news i am so glad for you so you will start your new job this coming monday on fri dec at yernagulahemanth <sub> good news <cont> hello yernagulahemanth casestudy no i need to finish my current projects in the sales department before i move <end>\n",
      "\n",
      "\n",
      "---------- actual ----------\n",
      "   finish my current projects in the sales department before i move  \n",
      "\n",
      "\n",
      "---------- predicted ----------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'finish my current projects <end> '"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ = generate_sent(195)\n",
    "print('-'*10,'input','-'*10)\n",
    "print(input_)\n",
    "print('\\n')\n",
    "print('-'*10,'actual','-'*10)\n",
    "print('   finish my current projects in the sales department before i move  ')\n",
    "print('\\n')\n",
    "print('-'*10,'predicted','-'*10)\n",
    "predict_next_word('<start> <to> yernagulahemanth <prv> hello yernagulahemanth wow this is great news i am so glad for you so you will start your new job this coming monday on fri dec at yernagulahemanth <sub> good news <cont> hello yernagulahemanth casestudy no i need to  <end>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "colab_type": "code",
    "id": "EHI5rKdxrUT2",
    "outputId": "70bad9c1-a312-4e22-f8e3-084b3d4391a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- input ----------\n",
      "<start> <to> yernagulahemanth <prv> nan <sub> opening in gms <cont> hello yernagulahemanth placements about company gms role data science intern location mumbai interviews to ml interviews requirements very good programming knowledge very good knowledge of ml and deep learning proven track record of applying deep learning and machine learning note preference would be given to people who <end>\n",
      "\n",
      "\n",
      "---------- actual ----------\n",
      " learning proven track record of applying deep learning and machine learning note preference would be given to people who  \n",
      "\n",
      "\n",
      "---------- predicted ----------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'learning proven track record of <end> '"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ = generate_sent(184)\n",
    "print('-'*10,'input','-'*10)\n",
    "print(input_)\n",
    "print('\\n')\n",
    "print('-'*10,'actual','-'*10)\n",
    "print(' learning proven track record of applying deep learning and machine learning note preference would be given to people who  ')\n",
    "print('\\n')\n",
    "print('-'*10,'predicted','-'*10)\n",
    "predict_next_word('<start> <to> yernagulahemanth <prv> nan <sub> opening in gms <cont> hello yernagulahemanth placements about company gms role data science intern location mumbai interviews to ml interviews requirements very good programming knowledge very good knowledge of ml and deep  <end>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "colab_type": "code",
    "id": "H43sCdA92PJq",
    "outputId": "08ec1ede-a493-4ffe-d71b-778d17475578"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- input ----------\n",
      "<start> <to> yernagulahemanth <prv> nan <sub> opening in gms <cont> hello yernagulahemanth placements about company gms role data science intern location mumbai interviews to ml interviews requirements very good programming knowledge very good knowledge of ml and deep learning proven track record of applying deep learning and machine learning note preference would be given to people who <end>\n",
      "\n",
      "\n",
      "---------- actual ----------\n",
      "  deep learning and machine learning note preference would be given to people who  \n",
      "\n",
      "\n",
      "---------- predicted ----------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'deep learning and machine <end> '"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ = generate_sent(184)\n",
    "print('-'*10,'input','-'*10)\n",
    "print(input_)\n",
    "print('\\n')\n",
    "print('-'*10,'actual','-'*10)\n",
    "print('  deep learning and machine learning note preference would be given to people who  ')\n",
    "print('\\n')\n",
    "print('-'*10,'predicted','-'*10)\n",
    "predict_next_word('<start> <to> yernagulahemanth <prv> nan <sub> opening in gms <cont> hello yernagulahemanth placements about company gms role data science intern location mumbai interviews to ml interviews requirements very good programming knowledge very good knowledge of ml and deep learning proven track record of applying <end>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "colab_type": "code",
    "id": "srblR3F62YkF",
    "outputId": "42d903de-25f4-408b-aa5d-129136783fba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- input ----------\n",
      "<start> <to> yernagulahemanth <prv> nan <sub> opening in gms <cont> hello yernagulahemanth placements about company gms role data science intern location mumbai interviews to ml interviews requirements very good programming knowledge very good knowledge of ml and deep learning proven track record of applying deep learning and machine learning note preference would be given to people who <end>\n",
      "\n",
      "\n",
      "---------- actual ----------\n",
      "  note preference would be given to people who  \n",
      "\n",
      "\n",
      "---------- predicted ----------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'note preference would be given <end> '"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ = generate_sent(184)\n",
    "print('-'*10,'input','-'*10)\n",
    "print(input_)\n",
    "print('\\n')\n",
    "print('-'*10,'actual','-'*10)\n",
    "print('  note preference would be given to people who  ')\n",
    "print('\\n')\n",
    "print('-'*10,'predicted','-'*10)\n",
    "predict_next_word('<start> <to> yernagulahemanth <prv> nan <sub> opening in gms <cont> hello yernagulahemanth placements about company gms role data science intern location mumbai interviews to ml interviews requirements very good programming knowledge very good knowledge of ml and deep learning proven track record of applying deep learning and machine learning <end>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "colab_type": "code",
    "id": "Ss3j9RO1mx1v",
    "outputId": "36f76802-3eaf-4746-ed3b-96d11ef11e3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- input ----------\n",
      "<start> <to> yernagulahemanth <prv> nan <sub> 22assign <cont> hello yernagulahemanth q ment rs applied ai course added a private comment on assignment apply svm on donors choose dataset m applied ai course performance of your models are poor can you please try out any techniques apart from the ones mentioned in the instructions and try to improve please incorporate the above suggestions and resubmit your assignment and also add a comment after resubmission reply if you dont want to receive emails from classroom you can unsubscribe google inc <end>\n",
      "\n",
      "\n",
      "---------- actual ----------\n",
      " resubmission reply if you dont want to receive emails from classroom you can unsubscribe google inc \n",
      "\n",
      "\n",
      "---------- predicted ----------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'also add a comment <end> '"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ = generate_sent(7)\n",
    "print('-'*10,'input','-'*10)\n",
    "print(input_)\n",
    "print('\\n')\n",
    "print('-'*10,'actual','-'*10)\n",
    "print(' resubmission reply if you dont want to receive emails from classroom you can unsubscribe google inc ')\n",
    "print('\\n')\n",
    "print('-'*10,'predicted','-'*10)\n",
    "predict_next_word('<start> <to> yernagulahemanth <prv> nan <sub> 22assign <cont> hello yernagulahemanth q ment rs applied ai course added a private comment on assignment apply svm on donors choose dataset m applied ai course performance of your models are poor can you please try out any techniques apart from the ones mentioned in the instructions and try to improve please incorporate the above suggestions and resubmit your assignment and also add a comment after  <end>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "colab_type": "code",
    "id": "PyG7EQtmnRZk",
    "outputId": "121068d3-6f34-4a95-a865-5dedbfffba71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- input ----------\n",
      "<start> <to> yernagulahemanth <prv> nan <sub> your coupons <cont> hello yernagulahemanth bookmyshow dear customer your coupons have been listed below if there is anything else we can do to make you smile please dont hesitate to get in touch with us off on pizzas on online ordering only ovenstory pizza coupon code bmsosfifty validity may may t c important instructions the coupons listed above are completely owned by the brands who are providing the offer bookmyshow will not be liable if the brands refuse to accept the coupons at any brand outlets please read all terms conditions of the respective coupon the coupons listed above can be used only once at a brand outlet any attempt <end>\n",
      "\n",
      "\n",
      "---------- actual ----------\n",
      "customer your coupons have been listed below if there is anything else we can do to make you smile please dont hesitate to get in touch with us off on pizzas on online ordering only ovenstory pizza coupon code bmsosfifty validity may may t c important instructions the coupons listed above are completely owned by the brands who are providing the offer bookmyshow will not be liable if the brands refuse to accept the coupons at any brand outlets please read all terms conditions of the respective coupon the coupons listed above can be used only once at a brand outlet any attempt\n",
      "\n",
      "\n",
      "---------- predicted ----------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'customer your coupons have <end> '"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ = generate_sent(20)\n",
    "print('-'*10,'input','-'*10)\n",
    "print(input_)\n",
    "print('\\n')\n",
    "print('-'*10,'actual','-'*10)\n",
    "print('customer your coupons have been listed below if there is anything else we can do to make you smile please dont hesitate to get in touch with us off on pizzas on online ordering only ovenstory pizza coupon code bmsosfifty validity may may t c important instructions the coupons listed above are completely owned by the brands who are providing the offer bookmyshow will not be liable if the brands refuse to accept the coupons at any brand outlets please read all terms conditions of the respective coupon the coupons listed above can be used only once at a brand outlet any attempt')\n",
    "print('\\n')\n",
    "print('-'*10,'predicted','-'*10)\n",
    "predict_next_word('<start> <to> yernagulahemanth <prv> nan <sub> your coupons <cont> hello yernagulahemanth bookmyshow dear   <end>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "colab_type": "code",
    "id": "ZdvO9cyjn85X",
    "outputId": "d36c9cb5-f6ff-47d0-fbdc-44becd645474"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- input ----------\n",
      "<start> <to> yernagulahemanth <prv> nan <sub> your coupons <cont> hello yernagulahemanth bookmyshow dear customer your coupons have been listed below if there is anything else we can do to make you smile please dont hesitate to get in touch with us off on pizzas on online ordering only ovenstory pizza coupon code bmsosfifty validity may may t c important instructions the coupons listed above are completely owned by the brands who are providing the offer bookmyshow will not be liable if the brands refuse to accept the coupons at any brand outlets please read all terms conditions of the respective coupon the coupons listed above can be used only once at a brand outlet any attempt <end>\n",
      "\n",
      "\n",
      "---------- actual ----------\n",
      " if there is anything else we can do to make you smile please dont hesitate to get in touch with us off on pizzas on online ordering only ovenstory pizza coupon code bmsosfifty validity may may t c important instructions the coupons listed above are completely owned by the brands who are providing the offer bookmyshow will not be liable if the brands refuse to accept the coupons at any brand outlets please read all terms conditions of the respective coupon the coupons listed above can be used only once at a brand outlet any attempt\n",
      "\n",
      "\n",
      "---------- predicted ----------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'there is anything else <end> '"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ = generate_sent(20)\n",
    "print('-'*10,'input','-'*10)\n",
    "print(input_)\n",
    "print('\\n')\n",
    "print('-'*10,'actual','-'*10)\n",
    "print(' if there is anything else we can do to make you smile please dont hesitate to get in touch with us off on pizzas on online ordering only ovenstory pizza coupon code bmsosfifty validity may may t c important instructions the coupons listed above are completely owned by the brands who are providing the offer bookmyshow will not be liable if the brands refuse to accept the coupons at any brand outlets please read all terms conditions of the respective coupon the coupons listed above can be used only once at a brand outlet any attempt')\n",
    "print('\\n')\n",
    "print('-'*10,'predicted','-'*10)\n",
    "predict_next_word('<start> <to> yernagulahemanth <prv> nan <sub> your coupons <cont> hello yernagulahemanth bookmyshow dear customer your coupons have been listed below  <end>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "colab_type": "code",
    "id": "4S7Gavms3eZM",
    "outputId": "2939105b-4985-4a00-c88c-aa94cd2699c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- input ----------\n",
      "<start> <to> yernagulahemanth <prv> nan <sub> your coupons <cont> hello yernagulahemanth bookmyshow dear customer your coupons have been listed below if there is anything else we can do to make you smile please dont hesitate to get in touch with us off on pizzas on online ordering only ovenstory pizza coupon code bmsosfifty validity may may t c important instructions the coupons listed above are completely owned by the brands who are providing the offer bookmyshow will not be liable if the brands refuse to accept the coupons at any brand outlets please read all terms conditions of the respective coupon the coupons listed above can be used only once at a brand outlet any attempt <end>\n",
      "\n",
      "\n",
      "---------- actual ----------\n",
      "  smile please dont hesitate to get in touch with us off on pizzas on online ordering only ovenstory pizza coupon code bmsosfifty validity may may t c important instructions the coupons listed above are completely owned by the brands who are providing the offer bookmyshow will not be liable if the brands refuse to accept the coupons at any brand outlets please read all terms conditions of the respective coupon the coupons listed above can be used only once at a brand outlet any attempt\n",
      "\n",
      "\n",
      "---------- predicted ----------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'dont hesitate to get <end> '"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ = generate_sent(20)\n",
    "print('-'*10,'input','-'*10)\n",
    "print(input_)\n",
    "print('\\n')\n",
    "print('-'*10,'actual','-'*10)\n",
    "print('  smile please dont hesitate to get in touch with us off on pizzas on online ordering only ovenstory pizza coupon code bmsosfifty validity may may t c important instructions the coupons listed above are completely owned by the brands who are providing the offer bookmyshow will not be liable if the brands refuse to accept the coupons at any brand outlets please read all terms conditions of the respective coupon the coupons listed above can be used only once at a brand outlet any attempt')\n",
    "print('\\n')\n",
    "print('-'*10,'predicted','-'*10)\n",
    "predict_next_word('<start> <to> yernagulahemanth <prv> nan <sub> your coupons <cont> hello yernagulahemanth bookmyshow dear customer your coupons have been listed below if there is anything else we can do to make you <end>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "colab_type": "code",
    "id": "J--Os4Dxn82r",
    "outputId": "c8d7a338-db57-47ff-be71-a58db1583820"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- input ----------\n",
      "<start> <to> yernagulahemanth <prv> nan <sub> your coupons <cont> hello yernagulahemanth bookmyshow dear customer your coupons have been listed below if there is anything else we can do to make you smile please dont hesitate to get in touch with us off on pizzas on online ordering only ovenstory pizza coupon code bmsosfifty validity may may t c important instructions the coupons listed above are completely owned by the brands who are providing the offer bookmyshow will not be liable if the brands refuse to accept the coupons at any brand outlets please read all terms conditions of the respective coupon the coupons listed above can be used only once at a brand outlet any attempt <end>\n",
      "\n",
      "\n",
      "---------- actual ----------\n",
      "  pizzas on online ordering only ovenstory pizza coupon code bmsosfifty validity may may t c important instructions the coupons listed above are completely owned by the brands who are providing the offer bookmyshow will not be liable if the brands refuse to accept the coupons at any brand outlets please read all terms conditions of the respective coupon the coupons listed above can be used only once at a brand outlet any attempt\n",
      "\n",
      "\n",
      "---------- predicted ----------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'pizza coupon code bmsosfifty validity <end> '"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ = generate_sent(20)\n",
    "print('-'*10,'input','-'*10)\n",
    "print(input_)\n",
    "print('\\n')\n",
    "print('-'*10,'actual','-'*10)\n",
    "print('  pizzas on online ordering only ovenstory pizza coupon code bmsosfifty validity may may t c important instructions the coupons listed above are completely owned by the brands who are providing the offer bookmyshow will not be liable if the brands refuse to accept the coupons at any brand outlets please read all terms conditions of the respective coupon the coupons listed above can be used only once at a brand outlet any attempt')\n",
    "print('\\n')\n",
    "print('-'*10,'predicted','-'*10)\n",
    "predict_next_word('<start> <to> yernagulahemanth <prv> nan <sub> your coupons <cont> hello yernagulahemanth bookmyshow dear customer your coupons have been listed below if there is anything else we can do to make you smile please dont hesitate to get in touch with us off on  <end>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "colab_type": "code",
    "id": "hy1K1Cu8n8zP",
    "outputId": "b55868ae-be31-459f-d931-4b86b7f98c49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- input ----------\n",
      "<start> <to> yernagulahemanth <prv> nan <sub> your coupons <cont> hello yernagulahemanth bookmyshow dear customer your coupons have been listed below if there is anything else we can do to make you smile please dont hesitate to get in touch with us off on pizzas on online ordering only ovenstory pizza coupon code bmsosfifty validity may may t c important instructions the coupons listed above are completely owned by the brands who are providing the offer bookmyshow will not be liable if the brands refuse to accept the coupons at any brand outlets please read all terms conditions of the respective coupon the coupons listed above can be used only once at a brand outlet any attempt <end>\n",
      "\n",
      "\n",
      "---------- actual ----------\n",
      "   the brands who are providing the offer bookmyshow will not be liable if the brands refuse to accept the coupons at any brand outlets please read all terms conditions of the respective coupon the coupons listed above can be used only once at a brand outlet any attempt\n",
      "\n",
      "\n",
      "---------- predicted ----------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'providing the offer bookmyshow <end> '"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ = generate_sent(20)\n",
    "print('-'*10,'input','-'*10)\n",
    "print(input_)\n",
    "print('\\n')\n",
    "print('-'*10,'actual','-'*10)\n",
    "print('   the brands who are providing the offer bookmyshow will not be liable if the brands refuse to accept the coupons at any brand outlets please read all terms conditions of the respective coupon the coupons listed above can be used only once at a brand outlet any attempt')\n",
    "print('\\n')\n",
    "print('-'*10,'predicted','-'*10)\n",
    "predict_next_word('<start> <to> yernagulahemanth <prv> nan <sub> your coupons <cont> hello yernagulahemanth bookmyshow dear customer your coupons have been listed below if there is anything else we can do to make you smile please dont hesitate to get in touch with us off on pizzas on online ordering only ovenstory pizza coupon code bmsosfifty validity may may t c important instructions the coupons listed above are completely owned by <end>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "colab_type": "code",
    "id": "bRwkaGh-oo2u",
    "outputId": "cc92ffa2-1df6-4d6a-c649-38191e75c83b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- input ----------\n",
      "<start> <to> yernagulahemanth <prv> check this out <sub> regarding deep learning project <cont> sir as we talked on the phone you asked me to use gcp utils gories you remember right even after using gcp the ram <end>\n",
      "\n",
      "\n",
      "---------- actual ----------\n",
      " talked on the phone you asked me to use gcp utils gories you remember right even after using gcp the ram \n",
      "\n",
      "\n",
      "---------- predicted ----------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'talked on <end> '"
      ]
     },
     "execution_count": 54,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ = generate_sent(38)\n",
    "print('-'*10,'input','-'*10)\n",
    "print(input_)\n",
    "print('\\n')\n",
    "print('-'*10,'actual','-'*10)\n",
    "print(' talked on the phone you asked me to use gcp utils gories you remember right even after using gcp the ram ')\n",
    "print('\\n')\n",
    "print('-'*10,'predicted','-'*10)\n",
    "predict_next_word('<start> <to> yernagulahemanth <prv> check this out <sub> regarding deep learning project <cont> sir as we  <end>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "colab_type": "code",
    "id": "g2OTfgM_oox9",
    "outputId": "14c2f659-11d0-4a2c-c2f1-bf4fecd671f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- input ----------\n",
      "<start> <to> yernagulahemanth <prv> check this out <sub> regarding deep learning project <cont> sir as we talked on the phone you asked me to use gcp utils gories you remember right even after using gcp the ram <end>\n",
      "\n",
      "\n",
      "---------- actual ----------\n",
      "  remember right even after using gcp the ram \n",
      "\n",
      "\n",
      "---------- predicted ----------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'gories you remember right <end> '"
      ]
     },
     "execution_count": 55,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ = generate_sent(38)\n",
    "print('-'*10,'input','-'*10)\n",
    "print(input_)\n",
    "print('\\n')\n",
    "print('-'*10,'actual','-'*10)\n",
    "print('  remember right even after using gcp the ram ')\n",
    "print('\\n')\n",
    "print('-'*10,'predicted','-'*10)\n",
    "predict_next_word('<start> <to> yernagulahemanth <prv> check this out <sub> regarding deep learning project <cont> sir as we talked on the phone you asked me to use gcp utils gories you <end>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "colab_type": "code",
    "id": "ls6cU_xwooqd",
    "outputId": "c655066d-f8ed-43cf-9671-d7e114e4076e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- input ----------\n",
      "<start> <to> yernagulahemanth <prv> check this out <sub> regarding deep learning project <cont> sir as we talked on the phone you asked me to use gcp utils gories you remember right even after using gcp the ram <end>\n",
      "\n",
      "\n",
      "---------- actual ----------\n",
      " using gcp the ram \n",
      "\n",
      "\n",
      "---------- predicted ----------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'using gcp <end> '"
      ]
     },
     "execution_count": 56,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ = generate_sent(38)\n",
    "print('-'*10,'input','-'*10)\n",
    "print(input_)\n",
    "print('\\n')\n",
    "print('-'*10,'actual','-'*10)\n",
    "print(' using gcp the ram ')\n",
    "print('\\n')\n",
    "print('-'*10,'predicted','-'*10)\n",
    "predict_next_word('<start> <to> yernagulahemanth <prv> check this out <sub> regarding deep learning project <cont> sir as we talked on the phone you asked me to use gcp utils gories you remember right even after <end>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "colab_type": "code",
    "id": "iAI21h8trUNv",
    "outputId": "81a6cb35-6266-4bfc-f6d2-f4625f0286e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- input ----------\n",
      "<start> <to> yernagulahemanth <prv> nan <sub> for our meeting tomorrow <cont> hello yernagulahemanth hey again ok so we ve finished our preparation for tomorrow s workshop and we re really looking forward to seeing you there during tomorrow s training we re going to be covering a lot of topics ranging from important <end>\n",
      "\n",
      "\n",
      "---------- actual ----------\n",
      "looking forward to seeing you there during tomorrow s training we re going to be covering a lot of topics ranging from important \n",
      "\n",
      "\n",
      "---------- predicted ----------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'to seeing you there during <end> '"
      ]
     },
     "execution_count": 59,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ = generate_sent(564)\n",
    "print('-'*10,'input','-'*10)\n",
    "print(input_)\n",
    "print('\\n')\n",
    "print('-'*10,'actual','-'*10)\n",
    "print('looking forward to seeing you there during tomorrow s training we re going to be covering a lot of topics ranging from important ')\n",
    "print('\\n')\n",
    "print('-'*10,'predicted','-'*10)\n",
    "predict_next_word('<start> <to> yernagulahemanth <prv> nan <sub> for our meeting tomorrow <cont> hello yernagulahemanth hey again ok so we ve finished our preparation for tomorrow s workshop and we re really  <end>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "colab_type": "code",
    "id": "FBmCWtHxsM5B",
    "outputId": "45d117b3-457c-48f8-a623-95d1a80ce661"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- input ----------\n",
      "<start> <to> yernagulahemanth <prv> nan <sub> for our meeting tomorrow <cont> hello yernagulahemanth hey again ok so we ve finished our preparation for tomorrow s workshop and we re really looking forward to seeing you there during tomorrow s training we re going to be covering a lot of topics ranging from important <end>\n",
      "\n",
      "\n",
      "---------- actual ----------\n",
      " again ok so we ve finished our preparation for tomorrow s workshop and we re really looking forward to seeing you there during tomorrow s  training we re going to be covering a lot of topics ranging from important \n",
      "\n",
      "\n",
      "---------- predicted ----------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'again ok so <end> '"
      ]
     },
     "execution_count": 61,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ = generate_sent(564)\n",
    "print('-'*10,'input','-'*10)\n",
    "print(input_)\n",
    "print('\\n')\n",
    "print('-'*10,'actual','-'*10)\n",
    "print(' again ok so we ve finished our preparation for tomorrow s workshop and we re really looking forward to seeing you there during tomorrow s  training we re going to be covering a lot of topics ranging from important ')\n",
    "print('\\n')\n",
    "print('-'*10,'predicted','-'*10)\n",
    "predict_next_word('<start> <to> yernagulahemanth <prv> nan <sub> for our meeting tomorrow <cont> hello yernagulahemanth hey <end>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NJGU2li6s5Vw"
   },
   "source": [
    "<strong> Most of the predictions are correct, every time model id predicting 3 or more words</strong> <br>NOTE: In actual section i have placed all next comming words(to make understad for readers) model only predicts minimum one word and maximum five words from the starting of actual section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tZC543DJskfM"
   },
   "source": [
    "## Steps Followed\n",
    "<ol>\n",
    "    <strong>Getting Data</strong>\n",
    "    <li>Get data from personal emails(each email is stored in notepad) \n",
    "        ref:<a href = \"https://www.youtube.com/watch?v=l02tTpOPNro\">Youtube</a></li>\n",
    "    <li>Extract to,subject,previous email(in case of replied email) and content and store them into a dataframe</li>\n",
    "    <li>Clean the data</li>\n",
    "    <strong> Preparing Data</strong>\n",
    "    <li>As shown in below table data is prepared </li>\n",
    "    <table>\n",
    "    <tr>\n",
    "      <th>Sentance</th>\n",
    "      <th>Output</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td> This </td>\n",
    "      <td> is</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td> This </td>\n",
    "      <td> is introduction </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td> This </td>\n",
    "      <td> is introduction to</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td> This </td>\n",
    "      <td> is introduction to my</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td> This </td>\n",
    "      <td> is introduction to my project</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td> This is </td>\n",
    "      <td> introduction</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td> This is</td>\n",
    "      <td> introduction to </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td> This is </td>\n",
    "      <td> introduction to my</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td> This is</td>\n",
    "      <td> introduction to my</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td> This is</td>\n",
    "      <td> introduction to my project</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td> This is introduction</td>\n",
    "      <td> to</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td> This is introduction</td>\n",
    "      <td>  to my </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td> This is introduction </td>\n",
    "      <td> to my project</td>\n",
    "    </tr>\n",
    " </table>\n",
    "    <li>Now to the sentance part,  I joined to, subject , previous email with there respective tags </li>\n",
    "    <u>for example:</u>\n",
    "    < start > < to > yernagulahemanth < prv > nan < sub > for our meeting tomorrow < cont > hello yernagulahemanth hey again ok so we ve finished our preparation for tomorrow s workshop and we re really looking forward to seeing you there during tomorrow s training we re going to be covering a lot of topics ranging from important < end >\n",
    "    <li>This sentance is fed into encoder to get hidden states and outputs</li>\n",
    "    <li>These hidden states and outputs are fed into attention layer to get context vector</li>\n",
    "    <li>This context vector passed to the decoder where final output is obtained</li>\n",
    "        \n",
    "</ol>        \n",
    "        \n",
    "<strong>For detailed explanation on model read this blog <a href=\"https://medium.com/@yernagulahemanth/google-smart-compose-46b289eca6bc\"> medium</a></strong> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gltkTugrmxnN"
   },
   "source": [
    "## Reference:\n",
    "<ol>\n",
    "    <li><a href=\"https://arxiv.org/pdf/1906.00080.pdf\">https://arxiv.org/pdf/1906.00080.pdf</a></li>\n",
    "    <li><a href=\"https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/4149/live-encoder-decoder-models/8/module-8-neural-networks-computer-vision-and-deep-learning\">AppliedAi Course(Encoder-Decoder )</a></li>\n",
    "    <li><a href=\"https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/4150/attention-models-in-deep-learning/8/module-8-neural-networks-computer-vision-and-deep-learning\">AppliedAi Course(Attention Based Models)</a></li>\n",
    "    <li><a href=\"https://blog.floydhub.com/attention-mechanism/\">Floydhub(Detailed explination of attention based models</a></li>\n",
    "    <li><a href=\"https://towardsdatascience.com/intuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f\">Language Translator with attention based model</a></li>\n",
    "        <li><a href=\"https://www.tensorflow.org/tutorials/text/nmt_with_attention\">Tensorflow implementation of attention based model</a>(Code Reference)</li>\n",
    "    <li><a href=\"https://machinelearningmastery.com/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks/\">Machine Learning Mastery</a></li>\n",
    "    <li><a href=\"https://www.coursera.org/lecture/nlp-sequence-models/attention-model-lSwVa\">Coursera</a></li>\n",
    "    <li><a href=\"https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/\">Analytics Vidhya</a></li>\n",
    "    \n",
    "</ol>    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hP-M7CgKZpFs"
   },
   "outputs": [],
   "source": [
    "# sent_ = '<start> <to> team <prv> nan <sub> about mentor <cont> hello team i recently completed my first mot interview will there be any mentor guiding us for the remaining part of our course till getting into the job if so when will <end>'\n",
    "# tokens = sent_.split()\n",
    "# inputs_ = []\n",
    "# to_pred = []\n",
    "# but_pred= []\n",
    "# new_q = ''\n",
    "# for i,word in enumerate(tokens):\n",
    "#     if i < len(tokens)-1:\n",
    "#         inputs_.append(new_q)\n",
    "#         to_pred.append(tokens[i+1])\n",
    "#         new_q = new_q + ' ' + tokens[i+1]\n",
    "#         pred = predict_next_word(u'<start> '+ new_q).replace('<end>','')\n",
    "        \n",
    "#         but_pred.append(pred)\n",
    "# dd = pd.DataFrame()\n",
    "# dd['inp'] = inputs_\n",
    "# dd['to_pred'] = to_pred\n",
    "# dd['but_pred']=but_pred\n",
    "\n",
    "# # for i in range(dd.shape[0]):\n",
    "# #     print(dd.inp.iloc[i]+' ::: '+dd.to_pred.iloc[i]+'  :::  '+dd.but_pred.iloc[i])\n",
    "\n",
    "\n",
    "# dd    "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "with_ATTENTION_gsp2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
